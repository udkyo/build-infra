#!/bin/bash -ex
TAG=$(date +%Y%m%d)

function error() {
    echo "$@"
    exit 1
}

# If an existing volume is present, we want to fail up front
echo "Checking to ensure no backup volume is present"
if [ "$(aws ec2 describe-volumes --region us-east-2 --filters Name=tag:Name,Values=gerrit-backup | jq -r '.Volumes[0]')" != "null" ]
then
  error "Backup volume already exists"
fi

# We'll trap EXIT and clean up the volume if it's been created
function cleanup() {
    for volume in $@
    do
        echo "Cleaning up ${volume}"
        sleep 1
        umount /mnt/{backup,scratch} &>/dev/null || true
        aws ec2 detach-volume --volume-id ${volume} --force --region ${REGION} || true
        sleep 5
        aws ec2 wait volume-available --volume-ids ${volume} --region ${REGION}
        aws ec2 delete-volume --volume-id ${volume} --region ${REGION}
    done
}

echo "Creating volumes"
export SNAPSHOT=$(aws ec2 describe-snapshots --filters "Name=volume-id,Values=${VOLUME_ID}"  "Name=status,Values=completed" --region ${REGION}  | jq -r '.[]|max_by(.StartTime)|.SnapshotId')
export BACKUP_VOLUME=$(/usr/local/bin/aws ec2 create-volume \
                --region ${REGION} \
                --volume-type gp3 \
                --iops 3000 \
                --throughput 200 \
                --snapshot-id ${SNAPSHOT} \
                --tag-specifications 'ResourceType=volume,Tags=[{Key=Name,Value=gerrit-backup},{Key=Owner,Value=build-team},{Key=Project,Value=gerrit},{Key=Purpose,Value=backup-restore}]' \
                --availability-zone ${REGION}a | jq -r '.VolumeId')
export SCRATCH_VOLUME=$(/usr/local/bin/aws ec2 create-volume \
                --region ${REGION} \
                --volume-type gp3 \
                --iops 3000 \
                --throughput 200 \
                --size 120 \
                --tag-specifications 'ResourceType=volume,Tags=[{Key=Name,Value=gerrit-scratch},{Key=Owner,Value=build-team},{Key=Project,Value=gerrit},{Key=Purpose,Value=backup-restore}]' \
                --availability-zone ${REGION}a | jq -r '.VolumeId')

trap "cleanup ${BACKUP_VOLUME} ${SCRATCH_VOLUME}" EXIT

# Check volumes
if [[ "${BACKUP_VOLUME}" = "" || "${SCRATCH_VOLUME}" = "" ]]
then
    error "Volume creation failed"
fi

echo "Attaching volumes"
aws ec2 wait volume-available --volume-ids ${BACKUP_VOLUME} --region ${REGION}
aws ec2 wait volume-available --volume-ids ${SCRATCH_VOLUME} --region ${REGION}
aws ec2 attach-volume --volume-id ${BACKUP_VOLUME} --instance-id `cat /var/lib/cloud/data/instance-id` --device /dev/sdf --region ${REGION}
aws ec2 attach-volume --volume-id ${SCRATCH_VOLUME} --instance-id `cat /var/lib/cloud/data/instance-id` --device /dev/sdg --region ${REGION}
aws ec2 wait volume-in-use --volume-ids ${BACKUP_VOLUME} --region ${REGION}
aws ec2 wait volume-in-use --volume-ids ${SCRATCH_VOLUME} --region ${REGION}
sleep 10
mkfs -t ext4 /dev/sdg

for vol in backup scratch
do
    if ! grep -qs "${vol}" /proc/mounts
    then
        mount /mnt/${vol}
    fi
done

cd /mnt/backup

echo "Backing up"
if ! tar -czf /mnt/scratch/backup-${TAG}.tgz \
    "cache" \
    "data" \
    "db" \
    "etc" \
    "git" \
    "index" \
    "lib" \
    "logs" \
    "plugins" \
    "static"
then
    error "Couldn't create tarball"
else
    aws s3 cp /mnt/scratch/backup-${TAG}.tgz s3://cb-gerrit.backups/ || \
        error "S3 upload failed"
fi

rm -rf /mnt/scratch/backup-${TAG}.tgz
